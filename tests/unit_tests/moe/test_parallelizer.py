# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys
import types
from unittest.mock import MagicMock

import pytest


class DummyParam:
    """Mock parameter with requires_grad attribute."""

    def __init__(self, requires_grad=True):
        self.requires_grad = requires_grad


class DummyExperts:
    def __init__(self):
        self._params = {"weight": DummyParam()}

    def named_parameters(self, recurse=False):
        for name, param in self._params.items():
            yield name, param

    def register_parameter(self, name, param):
        self._params[name] = param

    def parameters(self):
        for p in self._params.values():
            yield p


class DummyMoE:
    def __init__(self):
        self.experts = DummyExperts()


class DummyBlock:
    def __init__(self, mlp=None):
        self.mlp = mlp if mlp is not None else DummyMoE()


class LayerContainer:
    def __init__(self, blocks):
        self._blocks = blocks
        self.registered = {}

    def named_children(self):
        return [(str(i), b) for i, b in enumerate(self._blocks)]

    def register_module(self, name, module):
        self.registered[name] = module


class DummyModel:
    def __init__(self, blocks, embed_tokens=None, lm_head=None, audio_tower=None, visual=None):
        self.layers = LayerContainer(blocks)
        self.embed_tokens = embed_tokens
        self.lm_head = lm_head
        self.audio_tower = audio_tower
        self.visual = visual


def _install_torch_and_layers_stubs(monkeypatch):
    # Build minimal torch stub hierarchy
    torch_stub = types.ModuleType("torch")

    # nn submodule
    nn_stub = types.ModuleType("torch.nn")

    class Parameter:
        def __init__(self, data=None):
            self.data = data

    class Module:
        pass

    nn_stub.Parameter = Parameter
    nn_stub.Module = Module
    torch_stub.nn = nn_stub

    # cuda submodule
    cuda_stub = types.ModuleType("torch.cuda")

    class Stream:
        def __init__(self):
            pass

    cuda_stub.Stream = Stream
    torch_stub.cuda = cuda_stub

    # distributed submodules and symbols
    dist_stub = types.ModuleType("torch.distributed")

    # device_mesh
    device_mesh_stub = types.ModuleType("torch.distributed.device_mesh")

    class DeviceMesh:
        def __init__(self, *args, **kwargs):
            pass

    device_mesh_stub.DeviceMesh = DeviceMesh

    # fsdp
    fsdp_stub = types.ModuleType("torch.distributed.fsdp")

    def fully_shard(*args, **kwargs):
        return None

    fsdp_stub.fully_shard = fully_shard

    fsdp_fully_stub = types.ModuleType("torch.distributed.fsdp._fully_shard")

    class MixedPrecisionPolicy:
        def __init__(self, *args, **kwargs):
            pass

    class OffloadPolicy:
        def __init__(self, *args, **kwargs):
            pass

    fsdp_fully_stub.MixedPrecisionPolicy = MixedPrecisionPolicy
    fsdp_fully_stub.OffloadPolicy = OffloadPolicy

    # tensor
    tensor_stub = types.ModuleType("torch.distributed.tensor")

    def distribute_module(*args, **kwargs):
        return "DISTRIBUTED"

    def distribute_tensor(*args, **kwargs):
        return object()

    class Shard:
        def __init__(self, *args, **kwargs):
            pass

    tensor_stub.distribute_module = distribute_module
    tensor_stub.distribute_tensor = distribute_tensor
    tensor_stub.Shard = Shard

    # tensor.parallel
    tp_stub = types.ModuleType("torch.distributed.tensor.parallel")

    class ParallelStyle:
        pass

    def parallelize_module(*args, **kwargs):
        return None

    tp_stub.ParallelStyle = ParallelStyle
    tp_stub.parallelize_module = parallelize_module

    # algorithms._checkpoint.checkpoint_wrapper
    alg_stub = types.ModuleType("torch.distributed.algorithms")
    alg_cp_stub = types.ModuleType("torch.distributed.algorithms._checkpoint")
    cpw_stub = types.ModuleType(
        "torch.distributed.algorithms._checkpoint.checkpoint_wrapper"
    )

    def checkpoint_wrapper(*args, **kwargs):
        return args[0]

    cpw_stub.checkpoint_wrapper = checkpoint_wrapper

    # utils module hierarchy
    utils_stub = types.ModuleType("torch.utils")

    # utils.data
    utils_data_stub = types.ModuleType("torch.utils.data")

    class PinMemory:
        @staticmethod
        def _pin_memory_loop(*args, **kwargs):
            pass

        @staticmethod
        def pin_memory(*args, **kwargs):
            pass

    class DataUtils:
        pin_memory = PinMemory

    utils_data_stub._utils = DataUtils

    # utils.checkpoint
    utils_checkpoint_stub = types.ModuleType("torch.utils.checkpoint")

    class CheckpointPolicy:
        MUST_SAVE = 1
        PREFER_RECOMPUTE = 2

    def create_selective_checkpoint_contexts(policy_factory):
        return "CTX"

    utils_checkpoint_stub.CheckpointPolicy = CheckpointPolicy
    utils_checkpoint_stub.create_selective_checkpoint_contexts = (
        create_selective_checkpoint_contexts
    )

    # ops.aten.mm.default sentinel
    aten = types.SimpleNamespace(mm=types.SimpleNamespace(default=object()))
    torch_stub.ops = types.SimpleNamespace(aten=aten)

    # dtype and device classes for type annotations
    class dtype:
        pass

    class device:
        pass

    torch_stub.dtype = dtype
    torch_stub.device = device

    # common dtypes referenced by code
    torch_stub.bfloat16 = object()
    torch_stub.float32 = object()

    # register into sys.modules via monkeypatch
    monkeypatch.setitem(sys.modules, "torch", torch_stub)
    monkeypatch.setitem(sys.modules, "torch.nn", nn_stub)
    monkeypatch.setitem(sys.modules, "torch.cuda", cuda_stub)
    monkeypatch.setitem(sys.modules, "torch.distributed", dist_stub)
    monkeypatch.setitem(sys.modules, "torch.distributed.device_mesh", device_mesh_stub)
    monkeypatch.setitem(sys.modules, "torch.distributed.fsdp", fsdp_stub)
    monkeypatch.setitem(
        sys.modules,
        "torch.distributed.fsdp._fully_shard",
        fsdp_fully_stub,
    )
    monkeypatch.setitem(sys.modules, "torch.distributed.tensor", tensor_stub)
    monkeypatch.setitem(
        sys.modules, "torch.distributed.tensor.parallel", tp_stub
    )
    monkeypatch.setitem(sys.modules, "torch.distributed.algorithms", alg_stub)
    monkeypatch.setitem(
        sys.modules, "torch.distributed.algorithms._checkpoint", alg_cp_stub
    )
    monkeypatch.setitem(
        sys.modules,
        "torch.distributed.algorithms._checkpoint.checkpoint_wrapper",
        cpw_stub,
    )
    monkeypatch.setitem(sys.modules, "torch.utils", utils_stub)
    monkeypatch.setitem(sys.modules, "torch.utils.data", utils_data_stub)
    monkeypatch.setitem(sys.modules, "torch.utils.checkpoint", utils_checkpoint_stub)

    # Stub heavy layers import as well to avoid real dependencies
    layers_stub = types.ModuleType(
        "nemo_automodel.components.moe.layers"
    )

    class GroupedExpertsDeepEP:
        pass

    class MoE:
        pass

    layers_stub.GroupedExpertsDeepEP = GroupedExpertsDeepEP
    layers_stub.MoE = MoE
    monkeypatch.setitem(
        sys.modules, "nemo_automodel.components.moe.layers", layers_stub
    )

    # Stub experts module to avoid importing torch.nn.functional
    experts_stub = types.ModuleType(
        "nemo_automodel.components.moe.experts"
    )

    class GroupedExpertsTE:
        pass

    class GroupedExpertsFP8:
        pass

    experts_stub.GroupedExpertsDeepEP = GroupedExpertsDeepEP
    experts_stub.GroupedExpertsTE = GroupedExpertsTE
    experts_stub.GroupedExpertsFP8 = GroupedExpertsFP8
    monkeypatch.setitem(
        sys.modules, "nemo_automodel.components.moe.experts", experts_stub
    )


def _import_parallelizer_with_stubs(monkeypatch):
    import importlib

    # ensure fresh import of parallelizer
    for mod in [
        "nemo_automodel.components.moe.parallelizer",
        "nemo_automodel.components.moe.layers",
        "nemo_automodel.components.moe.experts",
        "nemo_automodel.components.distributed.pipelining",
        "nemo_automodel.components.distributed.pipelining.hf_utils",
    ]:
        if mod in sys.modules:
            sys.modules.pop(mod)

    _install_torch_and_layers_stubs(monkeypatch)

    # Stub the pipelining module and hf_utils
    pipelining_stub = types.ModuleType("nemo_automodel.components.distributed.pipelining")
    hf_utils_stub = types.ModuleType("nemo_automodel.components.distributed.pipelining.hf_utils")

    def get_text_module(model):
        """Return model.model if exists, otherwise model."""
        if hasattr(model, "model") and model.model is not None:
            return model.model
        return model

    hf_utils_stub.get_text_module = get_text_module
    pipelining_stub.hf_utils = hf_utils_stub

    monkeypatch.setitem(sys.modules, "nemo_automodel.components.distributed.pipelining", pipelining_stub)
    monkeypatch.setitem(sys.modules, "nemo_automodel.components.distributed.pipelining.hf_utils", hf_utils_stub)

    # Stub dtype_from_str utility
    shared_utils_stub = types.ModuleType("nemo_automodel.shared.utils")
    shared_utils_stub.dtype_from_str = lambda val, default=None: default
    monkeypatch.setitem(sys.modules, "nemo_automodel.shared.utils", shared_utils_stub)

    return importlib.import_module(
        "nemo_automodel.components.moe.parallelizer"
    )


def test_expert_parallel_apply_calls_distribute_module(monkeypatch):
    P = _import_parallelizer_with_stubs(monkeypatch)
    ep = P.ExpertParallel()
    module = DummyBlock().mlp.experts
    device_mesh = object()

    distribute_module_mock = MagicMock(return_value="DISTRIBUTED")
    monkeypatch.setattr(P, "distribute_module", distribute_module_mock)

    result = ep._apply(module, device_mesh)

    assert result == "DISTRIBUTED"
    assert distribute_module_mock.call_count == 1
    args, kwargs = distribute_module_mock.call_args
    # (module, device_mesh, partition_fn)
    assert args[0] is module
    assert args[1] is device_mesh
    assert callable(args[2])
    # ensure bound to same instance
    assert isinstance(args[2], types.MethodType) and args[2].__self__ is ep


def test_expert_parallel_partition_fn_shards_and_dispatcher(monkeypatch):
    P = _import_parallelizer_with_stubs(monkeypatch)
    # make the target module also look like GroupedExpertsDeepEP
    class DummyGrouped(DummyExperts):
        def __init__(self):
            super().__init__()
            self.dispatch_called_with = None

        def init_token_dispatcher(self, ep_mesh):
            self.dispatch_called_with = ep_mesh

        # override register_parameter to avoid strict type checks
        def register_parameter(self, name, param):
            setattr(self, name, param)

    # patch GroupedExpertsDeepEP symbol used in isinstance checks
    monkeypatch.setattr(P, "GroupedExpertsDeepEP", DummyGrouped)

    # mock distribute_tensor and Shard
    shard_sentinel = object()

    def fake_shard(dim):
        assert dim == 0
        return shard_sentinel

    distributed_obj = object()
    distribute_tensor_mock = MagicMock(return_value=distributed_obj)
    monkeypatch.setattr(P, "Shard", fake_shard)
    monkeypatch.setattr(P, "distribute_tensor", distribute_tensor_mock)

    ep = P.ExpertParallel()
    module = DummyGrouped()
    device_mesh = type("Mesh", (), {"ndim": 1})()

    # original parameter should exist
    assert any(True for _ in module.named_parameters(recurse=False))
    ep._partition_fn("any", module, device_mesh)

    # verify distribute_tensor was called for each top-level parameter with Shard(0)
    for _, param in module.named_parameters(recurse=False):
        pass  # push iterator once for coverage; we validate calls below

    assert distribute_tensor_mock.call_count >= 1
    for args, kwargs in distribute_tensor_mock.call_args_list:
        assert args[1] is device_mesh
        assert isinstance(args[2], list) and args[2][0] is shard_sentinel

    # dispatcher must be initialized
    assert module.dispatch_called_with is device_mesh


def test_expert_parallel_partition_fn_preserves_requires_grad(monkeypatch):
    """Test that _partition_fn preserves the requires_grad attribute of parameters."""
    P = _import_parallelizer_with_stubs(monkeypatch)

    class DummyExpertsWithGrad:
        def __init__(self):
            self._params = {}
            # Create parameters with different requires_grad values
            trainable_param = MagicMock()
            trainable_param.requires_grad = True
            frozen_param = MagicMock()
            frozen_param.requires_grad = False
            self._params["trainable_weight"] = trainable_param
            self._params["frozen_weight"] = frozen_param
            self.registered_params = {}

        def named_parameters(self, recurse=False):
            for name, param in self._params.items():
                yield name, param

        def register_parameter(self, name, param):
            self.registered_params[name] = param

    # Mock distribute_tensor to return a mock Parameter-like object
    def fake_distribute_tensor(param, device_mesh, placements):
        # Return a mock that can be wrapped in nn.Parameter
        mock_tensor = MagicMock()
        return mock_tensor

    monkeypatch.setattr(P, "distribute_tensor", fake_distribute_tensor)
    monkeypatch.setattr(P, "Shard", lambda dim: object())
    # Ensure module doesn't match GroupedExpertsDeepEP to skip dispatcher init
    monkeypatch.setattr(P, "GroupedExpertsDeepEP", type("NotMatching", (), {}))

    ep = P.ExpertParallel()
    module = DummyExpertsWithGrad()
    device_mesh = type("Mesh", (), {"ndim": 1})()

    ep._partition_fn("any", module, device_mesh)

    # Verify requires_grad is preserved for each registered parameter
    assert "trainable_weight" in module.registered_params
    assert "frozen_weight" in module.registered_params
    assert module.registered_params["trainable_weight"].requires_grad is True
    assert module.registered_params["frozen_weight"].requires_grad is False


def test_apply_ep_parallelizes_moe_experts(monkeypatch):
    P = _import_parallelizer_with_stubs(monkeypatch)
    # Patch MoE symbol for isinstance
    monkeypatch.setattr(P, "MoE", DummyMoE)
    parallelize_module_mock = MagicMock()
    monkeypatch.setattr(P, "parallelize_module", parallelize_module_mock)

    block = DummyBlock(mlp=DummyMoE())
    model = DummyModel([block])
    ep_mesh = type("Mesh", (), {"size": lambda self: 2})()

    P.apply_ep(model, ep_mesh)

    assert parallelize_module_mock.call_count == 1
    args, kwargs = parallelize_module_mock.call_args
    assert kwargs["module"] is block.mlp.experts
    assert kwargs["device_mesh"] is ep_mesh
    assert isinstance(kwargs["parallelize_plan"], P.ExpertParallel)


def test_apply_ac_wraps_blocks_with_and_without_context(monkeypatch):
    P = _import_parallelizer_with_stubs(monkeypatch)
    wrapper_returns = [object(), object()]

    def fake_wrapper(block, preserve_rng_state, context_fn=None):
        assert preserve_rng_state is True
        # if ignore_router=True, context_fn should be provided
        return wrapper_returns.pop(0)

    wrapper_mock = MagicMock(side_effect=fake_wrapper)
    ctx_mock = MagicMock(return_value="CTX")
    monkeypatch.setattr(P, "ptd_checkpoint_wrapper", wrapper_mock)
    monkeypatch.setattr(P, "create_selective_checkpoint_contexts", ctx_mock)

    blocks = [DummyBlock(), DummyBlock()]
    model = DummyModel(blocks)

    # ignore_router=True path - provide explicit hidden_size and num_experts
    P.apply_ac(model, ignore_router=True, hidden_size=7168, num_experts=256)
    assert wrapper_mock.call_count == 2
    # registration should replace both blocks
    assert len(model.layers.registered) == 2

    # reset for ignore_router=False path
    wrapper_returns.extend([object(), object()])
    model = DummyModel([DummyBlock(), DummyBlock()])
    wrapper_mock.reset_mock()
    model.layers.registered.clear()

    P.apply_ac(model, ignore_router=False, hidden_size=7168, num_experts=256)
    # context_fn should not be passed (3rd arg remains default None)
    for _, kwargs in wrapper_mock.call_args_list:
        assert "context_fn" not in kwargs or kwargs["context_fn"] is None
    assert len(model.layers.registered) == 2


def test_apply_ac_custom_policy_respects_hidden_and_expert_dims(monkeypatch):
    P = _import_parallelizer_with_stubs(monkeypatch)

    captured_policy = None

    def fake_create_selective_checkpoint_contexts(policy_cb):
        nonlocal captured_policy
        captured_policy = policy_cb
        return "CTX"

    def fake_wrapper(block, preserve_rng_state, context_fn=None):
        assert preserve_rng_state is True
        assert callable(context_fn)
        assert context_fn() == "CTX"
        return block

    monkeypatch.setattr(P, "create_selective_checkpoint_contexts", fake_create_selective_checkpoint_contexts)
    monkeypatch.setattr(P, "ptd_checkpoint_wrapper", MagicMock(side_effect=fake_wrapper))

    hidden_size = 17
    num_experts = 31
    model = DummyModel([DummyBlock(), DummyBlock()])

    P.apply_ac(model, ignore_router=True, hidden_size=hidden_size, num_experts=num_experts)

    assert captured_policy is not None

    torch_stub = sys.modules["torch"]
    rhs_match = type("Mat", (), {"shape": (hidden_size, num_experts)})()
    rhs_mismatch = type("Mat", (), {"shape": (hidden_size, num_experts + 1)})()

    policy = captured_policy
    must_save = policy(None, torch_stub.ops.aten.mm.default, object(), rhs_match)
    prefer_recompute_shape = policy(None, torch_stub.ops.aten.mm.default, object(), rhs_mismatch)
    prefer_recompute_func = policy(None, object(), object(), rhs_match)

    assert must_save == P.CheckpointPolicy.MUST_SAVE
    assert prefer_recompute_shape == P.CheckpointPolicy.PREFER_RECOMPUTE
    assert prefer_recompute_func == P.CheckpointPolicy.PREFER_RECOMPUTE


def _find_call_by_first_arg(mock_obj, target_first_arg):
    for args, kwargs in mock_obj.call_args_list:
        if args and args[0] is target_first_arg:
            return args, kwargs
    return None


def test_apply_fsdp_calls_with_ignored_params_and_shard_for_experts(monkeypatch):
    P = _import_parallelizer_with_stubs(monkeypatch)
    # Patch MoE symbol for isinstance
    monkeypatch.setattr(P, "MoE", DummyMoE)

    fully_shard_mock = MagicMock()
    mp_policy_mock = MagicMock(return_value="MP_POLICY")
    shard_sentinel = object()

    def fake_shard(dim):
        assert dim == 1
        return shard_sentinel

    monkeypatch.setattr(P, "fully_shard", fully_shard_mock)
    monkeypatch.setattr(P, "MixedPrecisionPolicy", mp_policy_mock)
    monkeypatch.setattr(P, "Shard", fake_shard)

    block = DummyBlock(mlp=DummyMoE())
    embed = object()
    lm = object()
    model = DummyModel([block], embed_tokens=embed, lm_head=lm)

    fsdp_mesh = type("Mesh", (), {"size": lambda self: 2})()
    ep_shard_mesh = type("Mesh", (), {"size": lambda self: 2})()

    P.apply_fsdp(
        model=model,
        fsdp_mesh=fsdp_mesh,
        ep_enabled=True,
        ep_shard_enabled=True,
        ep_shard_mesh=ep_shard_mesh,
    )

    # Experts should have a dedicated shard call
    experts = block.mlp.experts
    experts_call = _find_call_by_first_arg(fully_shard_mock, experts)
    assert experts_call is not None
    _, experts_kwargs = experts_call
    assert experts_kwargs["mesh"] is ep_shard_mesh
    assert experts_kwargs["reshard_after_forward"] is False
    assert callable(experts_kwargs["shard_placement_fn"])  # lambda _: Shard(1)

    # Block should be sharded with ignored_params when ep_enabled
    block_call = _find_call_by_first_arg(fully_shard_mock, block)
    assert block_call is not None
    _, block_kwargs = block_call
    assert block_kwargs["mesh"] is fsdp_mesh
    assert block_kwargs["mp_policy"] == "MP_POLICY"
    ignored = block_kwargs.get("ignored_params")
    assert isinstance(ignored, set) and len(ignored) == len(list(experts.parameters()))

    # embed, lm_head and model should also be sharded on fsdp_mesh
    embed_call = _find_call_by_first_arg(fully_shard_mock, embed)
    assert embed_call is not None and embed_call[1]["mesh"] is fsdp_mesh

    lm_call = _find_call_by_first_arg(fully_shard_mock, lm)
    assert lm_call is not None and lm_call[1]["mesh"] is fsdp_mesh

    model_call = _find_call_by_first_arg(fully_shard_mock, model)
    assert model_call is not None and model_call[1]["mesh"] is fsdp_mesh


def test_apply_fsdp_without_ep_enabled_has_no_ignored_params(monkeypatch):
    P = _import_parallelizer_with_stubs(monkeypatch)
    monkeypatch.setattr(P, "MoE", DummyMoE)
    fully_shard_mock = MagicMock()
    monkeypatch.setattr(P, "fully_shard", fully_shard_mock)
    monkeypatch.setattr(P, "MixedPrecisionPolicy", MagicMock(return_value="MP_POLICY"))

    block = DummyBlock(mlp=DummyMoE())
    model = DummyModel([block])
    fsdp_mesh = object()

    P.apply_fsdp(
        model=model,
        fsdp_mesh=fsdp_mesh,
        ep_enabled=False,
        ep_shard_enabled=False,
        ep_shard_mesh=None,
    )

    block_call = _find_call_by_first_arg(fully_shard_mock, block)
    assert block_call is not None
    _, block_kwargs = block_call
    assert block_kwargs["mesh"] is fsdp_mesh
    assert block_kwargs.get("ignored_params") is None


@pytest.mark.parametrize(
    "audio_trainable, visual_trainable",
    [
        (True, True),
        (False, False),
    ],
)
def test_apply_fsdp_handles_multimodal_components(monkeypatch, audio_trainable, visual_trainable):
    P = _import_parallelizer_with_stubs(monkeypatch)
    monkeypatch.setattr(P, "MoE", DummyMoE)
    fully_shard_mock = MagicMock()
    monkeypatch.setattr(P, "fully_shard", fully_shard_mock)
    monkeypatch.setattr(P, "MixedPrecisionPolicy", MagicMock(return_value="MP_POLICY"))

    logging_mock = MagicMock()
    monkeypatch.setattr(P.logging, "info", logging_mock)

    class Tower:
        def __init__(self, requires_grad):
            self._params = [types.SimpleNamespace(requires_grad=requires_grad)]

        def parameters(self):
            return iter(self._params)

    audio_tower = Tower(audio_trainable)
    visual_tower = Tower(visual_trainable)

    model = DummyModel([DummyBlock()], audio_tower=audio_tower, visual=visual_tower)

    P.apply_fsdp(
        model=model,
        fsdp_mesh=object(),
        ep_enabled=False,
        ep_shard_enabled=False,
        ep_shard_mesh=None,
    )

    audio_call = _find_call_by_first_arg(fully_shard_mock, audio_tower)
    visual_call = _find_call_by_first_arg(fully_shard_mock, visual_tower)
    assert (audio_call is not None) == audio_trainable
    assert (visual_call is not None) == visual_trainable

    if not audio_trainable:
        logging_mock.assert_any_call("Skipping FSDP wrap for frozen audio tower")
    if not visual_trainable:
        logging_mock.assert_any_call("Skipping FSDP wrap for frozen visual tower")


class MeshView:
    def __init__(self, size):
        self._size = size

    def size(self):
        return self._size


class FakeWorldMesh:
    def __init__(self, sizes_by_key, mesh_dim_names):
        self._sizes = sizes_by_key
        self.mesh_dim_names = set(mesh_dim_names)

    def __getitem__(self, key):
        return MeshView(self._sizes[key])


class FakeMoeMesh:
    def __init__(self, sizes_by_key):
        self._sizes = sizes_by_key

    def __getitem__(self, key):
        return MeshView(self._sizes[key])


def test_parallelize_model_calls_subsystems_and_validates(monkeypatch):
    P = _import_parallelizer_with_stubs(monkeypatch)
    apply_ep_mock = MagicMock()
    apply_ac_mock = MagicMock()
    apply_fsdp_mock = MagicMock()
    monkeypatch.setattr(P, "apply_ep", apply_ep_mock)
    monkeypatch.setattr(P, "apply_ac", apply_ac_mock)
    monkeypatch.setattr(P, "apply_fsdp", apply_fsdp_mock)

    world_mesh = FakeWorldMesh({("dp",): 2, "tp": 1, "cp": 1}, mesh_dim_names=["dp", "tp", "cp"])
    moe_mesh = FakeMoeMesh({"ep": 2, ("es1", "es2"): 2})

    # model.model.moe_config.n_routed_experts must be divisible by ep size (2)
    class Inner:
        def __init__(self):
            self.moe_config = type("MC", (), {"n_routed_experts": 4})()

    class Outer:
        def __init__(self):
            self.model = Inner()

    model = Outer()

    P.parallelize_model(
        model=model,
        world_mesh=world_mesh,
        moe_mesh=moe_mesh,
        dp_axis_names=("dp",),
        cp_axis_name=None,
        tp_axis_name=None,
        ep_axis_name="ep",
        ep_shard_axis_names=("es1", "es2"),
        activation_checkpointing=True,
    )
    apply_ep_mock.assert_called_once()
    # AC enabled
    apply_ac_mock.assert_called_once_with(model, ignore_router=False)
    # FSDP called with combined flags and derived meshes
    args, kwargs = apply_fsdp_mock.call_args
    # handle positional or keyword invocations
    fsdp_model = kwargs.get("model", args[0] if args else None)
    fsdp_mesh_arg = kwargs.get("fsdp_mesh", args[1] if len(args) > 1 else None)
    ep_enabled = kwargs.get("ep_enabled", args[2] if len(args) > 2 else None)
    ep_shard_enabled = kwargs.get("ep_shard_enabled", args[3] if len(args) > 3 else None)
    ep_shard_mesh_arg = kwargs.get("ep_shard_mesh", args[4] if len(args) > 4 else None)

    assert fsdp_model is model
    assert fsdp_mesh_arg.size() == 2
    assert ep_enabled is True
    assert ep_shard_enabled is True
    assert ep_shard_mesh_arg.size() == 2


def test_parallelize_model_asserts_on_invalid_tp_cp_and_ep_divisibility(monkeypatch):
    P = _import_parallelizer_with_stubs(monkeypatch)
    world_mesh_bad_tp = FakeWorldMesh({"tp": 2, "cp": 1}, mesh_dim_names=["tp", "cp"])
    moe_mesh = FakeMoeMesh({"ep": 2})

    class Inner:
        def __init__(self):
            self.moe_config = type("MC", (), {"n_routed_experts": 3})()  # not divisible by 2

    class Outer:
        def __init__(self):
            self.model = Inner()

    model = Outer()

    # TP size != 1 -> assertion
    with pytest.raises(AssertionError):
        P.parallelize_model(
            model=model,
            world_mesh=world_mesh_bad_tp,
            moe_mesh=moe_mesh,
            dp_axis_names=None,
            cp_axis_name=None,
            tp_axis_name="tp",
            ep_axis_name=None,
            ep_shard_axis_names=None,
            activation_checkpointing=False,
        )

    # EP enabled but divisibility violated -> assertion
    world_mesh_ok = FakeWorldMesh({("dp",): 1, "tp": 1, "cp": 1}, mesh_dim_names=["dp", "tp", "cp"])
    moe_mesh_ep = FakeMoeMesh({"ep": 2})
    with pytest.raises(AssertionError):
        P.parallelize_model(
            model=model,
            world_mesh=world_mesh_ok,
            moe_mesh=moe_mesh_ep,
            dp_axis_names=("dp",),
            cp_axis_name=None,
            tp_axis_name=None,
            ep_axis_name="ep",
            ep_shard_axis_names=None,
            activation_checkpointing=False,
        )


def test_apply_fsdp_with_lm_head_precision_fp32(monkeypatch):
    """Test that apply_fsdp applies custom MixedPrecisionPolicy to lm_head when lm_head_precision is fp32."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    monkeypatch.setattr(P, "MoE", DummyMoE)

    fully_shard_mock = MagicMock()
    mp_policy_mock = MagicMock(return_value="MP_POLICY")
    monkeypatch.setattr(P, "fully_shard", fully_shard_mock)
    monkeypatch.setattr(P, "MixedPrecisionPolicy", mp_policy_mock)

    torch_stub = sys.modules["torch"]
    block = DummyBlock(mlp=DummyMoE())
    lm = object()
    model = DummyModel([block], lm_head=lm)
    fsdp_mesh = object()

    P.apply_fsdp(
        model=model,
        fsdp_mesh=fsdp_mesh,
        ep_enabled=False,
        ep_shard_enabled=False,
        lm_head_precision=torch_stub.float32,
    )

    # Find the lm_head call
    lm_call = _find_call_by_first_arg(fully_shard_mock, lm)
    assert lm_call is not None
    _, lm_kwargs = lm_call

    # Verify custom MixedPrecisionPolicy was created with fp32 for all dtypes
    assert mp_policy_mock.call_count >= 2  # default + lm_head
    # Find the call for lm_head's custom policy
    fp32_policy_calls = [
        call for call in mp_policy_mock.call_args_list
        if call[1].get("param_dtype") == torch_stub.float32
        and call[1].get("reduce_dtype") == torch_stub.float32
        and call[1].get("output_dtype") == torch_stub.float32
    ]
    assert len(fp32_policy_calls) == 1


def test_apply_fsdp_without_lm_head_precision_uses_default_policy(monkeypatch):
    """Test that apply_fsdp uses default MixedPrecisionPolicy for lm_head when lm_head_precision is None."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    monkeypatch.setattr(P, "MoE", DummyMoE)

    fully_shard_mock = MagicMock()
    mp_policy_mock = MagicMock(return_value="MP_POLICY")
    monkeypatch.setattr(P, "fully_shard", fully_shard_mock)
    monkeypatch.setattr(P, "MixedPrecisionPolicy", mp_policy_mock)

    block = DummyBlock(mlp=DummyMoE())
    lm = object()
    model = DummyModel([block], lm_head=lm)
    fsdp_mesh = object()

    P.apply_fsdp(
        model=model,
        fsdp_mesh=fsdp_mesh,
        ep_enabled=False,
        ep_shard_enabled=False,
        lm_head_precision=None,
    )

    # Find the lm_head call
    lm_call = _find_call_by_first_arg(fully_shard_mock, lm)
    assert lm_call is not None

    # Should only have one MixedPrecisionPolicy call (the default one)
    assert mp_policy_mock.call_count == 1


def test_parallelize_model_passes_lm_head_precision_to_apply_fsdp(monkeypatch):
    """Test that parallelize_model passes lm_head_precision to apply_fsdp."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    apply_fsdp_mock = MagicMock()
    monkeypatch.setattr(P, "apply_fsdp", apply_fsdp_mock)
    monkeypatch.setattr(P, "apply_ep", MagicMock())
    monkeypatch.setattr(P, "apply_ac", MagicMock())

    world_mesh = FakeWorldMesh({("dp",): 2}, mesh_dim_names=["dp"])
    moe_mesh = None

    torch_stub = sys.modules["torch"]

    class Inner:
        def __init__(self):
            self.moe_config = type("MC", (), {"n_routed_experts": 4})()

    class Outer:
        def __init__(self):
            self.model = Inner()

    model = Outer()

    P.parallelize_model(
        model=model,
        world_mesh=world_mesh,
        moe_mesh=moe_mesh,
        dp_axis_names=("dp",),
        lm_head_precision=torch_stub.float32,
    )

    # Verify apply_fsdp was called with lm_head_precision
    apply_fsdp_mock.assert_called_once()
    _, kwargs = apply_fsdp_mock.call_args
    assert kwargs.get("lm_head_precision") == torch_stub.float32


def test_apply_fsdp_with_lm_head_precision_string_input(monkeypatch):
    """Test that apply_fsdp accepts string input for lm_head_precision and converts to torch.dtype."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    monkeypatch.setattr(P, "MoE", DummyMoE)

    fully_shard_mock = MagicMock()
    mp_policy_mock = MagicMock(return_value="MP_POLICY")
    monkeypatch.setattr(P, "fully_shard", fully_shard_mock)
    monkeypatch.setattr(P, "MixedPrecisionPolicy", mp_policy_mock)

    torch_stub = sys.modules["torch"]

    # Mock dtype_from_str to convert string to torch.float32
    def mock_dtype_from_str(val, default=None):
        if val == "float32" or val == "torch.float32":
            return torch_stub.float32
        return default

    monkeypatch.setattr(P, "dtype_from_str", mock_dtype_from_str)

    block = DummyBlock(mlp=DummyMoE())
    lm = object()
    model = DummyModel([block], lm_head=lm)
    fsdp_mesh = object()

    P.apply_fsdp(
        model=model,
        fsdp_mesh=fsdp_mesh,
        ep_enabled=False,
        ep_shard_enabled=False,
        lm_head_precision="float32",
    )

    # Find the lm_head call
    lm_call = _find_call_by_first_arg(fully_shard_mock, lm)
    assert lm_call is not None

    # Verify custom MixedPrecisionPolicy was created with fp32 for all dtypes
    assert mp_policy_mock.call_count >= 2  # default + lm_head
    # Find the call for lm_head's custom policy
    fp32_policy_calls = [
        call for call in mp_policy_mock.call_args_list
        if call[1].get("param_dtype") == torch_stub.float32
        and call[1].get("reduce_dtype") == torch_stub.float32
        and call[1].get("output_dtype") == torch_stub.float32
    ]
    assert len(fp32_policy_calls) == 1


def test_parallelize_model_with_lm_head_precision_string_input(monkeypatch):
    """Test that parallelize_model accepts string input for lm_head_precision."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    apply_fsdp_mock = MagicMock()
    monkeypatch.setattr(P, "apply_fsdp", apply_fsdp_mock)
    monkeypatch.setattr(P, "apply_ep", MagicMock())
    monkeypatch.setattr(P, "apply_ac", MagicMock())

    world_mesh = FakeWorldMesh({("dp",): 2}, mesh_dim_names=["dp"])
    moe_mesh = None

    class Inner:
        def __init__(self):
            self.moe_config = type("MC", (), {"n_routed_experts": 4})()

    class Outer:
        def __init__(self):
            self.model = Inner()

    model = Outer()

    P.parallelize_model(
        model=model,
        world_mesh=world_mesh,
        moe_mesh=moe_mesh,
        dp_axis_names=("dp",),
        lm_head_precision="float32",
    )

    # Verify apply_fsdp was called with lm_head_precision as a string
    apply_fsdp_mock.assert_called_once()
    _, kwargs = apply_fsdp_mock.call_args
    assert kwargs.get("lm_head_precision") == "float32"


def test_apply_fsdp_with_wrap_outer_model_true(monkeypatch):
    """Test that apply_fsdp wraps both inner _model and outer model when wrap_outer_model=True."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    monkeypatch.setattr(P, "MoE", DummyMoE)

    fully_shard_mock = MagicMock()
    monkeypatch.setattr(P, "fully_shard", fully_shard_mock)
    monkeypatch.setattr(P, "MixedPrecisionPolicy", MagicMock(return_value="MP_POLICY"))

    block = DummyBlock(mlp=DummyMoE())
    # Create a model with nested structure (model.model exists)
    inner_model = DummyModel([block])

    class OuterModel:
        def __init__(self, inner):
            self.model = inner

    outer_model = OuterModel(inner_model)
    fsdp_mesh = object()

    P.apply_fsdp(
        model=outer_model,
        fsdp_mesh=fsdp_mesh,
        ep_enabled=False,
        ep_shard_enabled=False,
        wrap_outer_model=True,
    )

    # Find calls for inner model and outer model
    inner_call = _find_call_by_first_arg(fully_shard_mock, inner_model)
    outer_call = _find_call_by_first_arg(fully_shard_mock, outer_model)

    # Both should be wrapped
    assert inner_call is not None, "Inner model should be wrapped"
    assert outer_call is not None, "Outer model should be wrapped when wrap_outer_model=True"


def test_apply_fsdp_with_wrap_outer_model_false(monkeypatch):
    """Test that apply_fsdp only wraps inner _model when wrap_outer_model=False (default)."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    monkeypatch.setattr(P, "MoE", DummyMoE)

    fully_shard_mock = MagicMock()
    monkeypatch.setattr(P, "fully_shard", fully_shard_mock)
    monkeypatch.setattr(P, "MixedPrecisionPolicy", MagicMock(return_value="MP_POLICY"))

    block = DummyBlock(mlp=DummyMoE())
    # Create a model with nested structure (model.model exists)
    inner_model = DummyModel([block])

    class OuterModel:
        def __init__(self, inner):
            self.model = inner

    outer_model = OuterModel(inner_model)
    fsdp_mesh = object()

    P.apply_fsdp(
        model=outer_model,
        fsdp_mesh=fsdp_mesh,
        ep_enabled=False,
        ep_shard_enabled=False,
        wrap_outer_model=False,
    )

    # Find calls for inner model and outer model
    inner_call = _find_call_by_first_arg(fully_shard_mock, inner_model)
    outer_call = _find_call_by_first_arg(fully_shard_mock, outer_model)

    # Only inner should be wrapped
    assert inner_call is not None, "Inner model should be wrapped"
    assert outer_call is None, "Outer model should NOT be wrapped when wrap_outer_model=False"


def test_apply_fsdp_wrap_outer_model_no_nested_structure(monkeypatch):
    """Test that wrap_outer_model has no effect when model has no nested structure."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    monkeypatch.setattr(P, "MoE", DummyMoE)

    fully_shard_mock = MagicMock()
    monkeypatch.setattr(P, "fully_shard", fully_shard_mock)
    monkeypatch.setattr(P, "MixedPrecisionPolicy", MagicMock(return_value="MP_POLICY"))

    block = DummyBlock(mlp=DummyMoE())
    # Create a model without nested structure (no model.model)
    model = DummyModel([block])
    fsdp_mesh = object()

    P.apply_fsdp(
        model=model,
        fsdp_mesh=fsdp_mesh,
        ep_enabled=False,
        ep_shard_enabled=False,
        wrap_outer_model=True,
    )

    # Find call for model
    model_call = _find_call_by_first_arg(fully_shard_mock, model)

    # Model should be wrapped exactly once (not twice)
    assert model_call is not None, "Model should be wrapped"
    # Count how many times model was passed as first arg
    model_call_count = sum(
        1 for args, _ in fully_shard_mock.call_args_list if args and args[0] is model
    )
    assert model_call_count == 1, "Model should only be wrapped once when model == _model"


def test_parallelize_model_passes_wrap_outer_model_to_apply_fsdp(monkeypatch):
    """Test that parallelize_model passes wrap_outer_model to apply_fsdp."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    apply_fsdp_mock = MagicMock()
    monkeypatch.setattr(P, "apply_fsdp", apply_fsdp_mock)
    monkeypatch.setattr(P, "apply_ep", MagicMock())
    monkeypatch.setattr(P, "apply_ac", MagicMock())

    world_mesh = FakeWorldMesh({("dp",): 2}, mesh_dim_names=["dp"])
    moe_mesh = None

    class Inner:
        def __init__(self):
            self.moe_config = type("MC", (), {"n_routed_experts": 4})()

    class Outer:
        def __init__(self):
            self.model = Inner()

    model = Outer()

    P.parallelize_model(
        model=model,
        world_mesh=world_mesh,
        moe_mesh=moe_mesh,
        dp_axis_names=("dp",),
        wrap_outer_model=True,
    )

    # Verify apply_fsdp was called with wrap_outer_model=True
    apply_fsdp_mock.assert_called_once()
    _, kwargs = apply_fsdp_mock.call_args
    assert kwargs.get("wrap_outer_model") is True


def test_parallelize_model_wrap_outer_model_defaults_to_true(monkeypatch):
    """Test that parallelize_model defaults wrap_outer_model to True."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    apply_fsdp_mock = MagicMock()
    monkeypatch.setattr(P, "apply_fsdp", apply_fsdp_mock)
    monkeypatch.setattr(P, "apply_ep", MagicMock())
    monkeypatch.setattr(P, "apply_ac", MagicMock())

    world_mesh = FakeWorldMesh({("dp",): 2}, mesh_dim_names=["dp"])
    moe_mesh = None

    class Inner:
        def __init__(self):
            self.moe_config = type("MC", (), {"n_routed_experts": 4})()

    class Outer:
        def __init__(self):
            self.model = Inner()

    model = Outer()

    P.parallelize_model(
        model=model,
        world_mesh=world_mesh,
        moe_mesh=moe_mesh,
        dp_axis_names=("dp",),
    )

    # Verify apply_fsdp was called with wrap_outer_model=False (default)
    apply_fsdp_mock.assert_called_once()
    _, kwargs = apply_fsdp_mock.call_args
    assert kwargs.get("wrap_outer_model") is True


def test_apply_ac_derives_hidden_size_and_num_experts_from_config(monkeypatch):
    """Test that apply_ac derives hidden_size and num_experts from model.config."""
    P = _import_parallelizer_with_stubs(monkeypatch)

    captured_hidden_size = None
    captured_num_experts = None

    def fake_create_selective_checkpoint_contexts(policy_cb):
        nonlocal captured_hidden_size, captured_num_experts
        # Extract hidden_size and num_experts by testing the policy
        torch_stub = sys.modules["torch"]
        # Test with various shapes to determine what was captured
        for hs in [128, 256, 512, 1024]:
            for ne in [8, 16, 32, 64]:
                rhs = type("Mat", (), {"shape": (hs, ne)})()
                result = policy_cb(None, torch_stub.ops.aten.mm.default, object(), rhs)
                if result == P.CheckpointPolicy.MUST_SAVE:
                    captured_hidden_size = hs
                    captured_num_experts = ne
                    break
            if captured_hidden_size is not None:
                break
        return "CTX"

    def fake_wrapper(block, preserve_rng_state, context_fn=None):
        if context_fn is not None:
            context_fn()  # Trigger the context function to capture values
        return block

    monkeypatch.setattr(P, "create_selective_checkpoint_contexts", fake_create_selective_checkpoint_contexts)
    monkeypatch.setattr(P, "ptd_checkpoint_wrapper", MagicMock(side_effect=fake_wrapper))

    # Create model with config containing hidden_size and num_experts
    class Config:
        hidden_size = 256
        num_experts = 16

    class ModelWithConfig:
        def __init__(self):
            self.config = Config()
            self.layers = LayerContainer([DummyBlock()])

    model = ModelWithConfig()

    P.apply_ac(model, ignore_router=True)

    assert captured_hidden_size == 256
    assert captured_num_experts == 16


def test_apply_ac_raises_when_hidden_size_not_available(monkeypatch):
    """Test that apply_ac raises ValueError when hidden_size is not in config and not provided."""
    P = _import_parallelizer_with_stubs(monkeypatch)

    # Model without config
    class ModelWithoutConfig:
        def __init__(self):
            self.layers = LayerContainer([DummyBlock()])

    model = ModelWithoutConfig()

    with pytest.raises(ValueError, match="hidden_size must be provided"):
        P.apply_ac(model)


def test_apply_ac_raises_when_num_experts_not_available(monkeypatch):
    """Test that apply_ac raises ValueError when num_experts is not in config and not provided."""
    P = _import_parallelizer_with_stubs(monkeypatch)

    # Model with config containing only hidden_size
    class ConfigPartial:
        hidden_size = 256

    class ModelPartialConfig:
        def __init__(self):
            self.config = ConfigPartial()
            self.layers = LayerContainer([DummyBlock()])

    model = ModelPartialConfig()

    with pytest.raises(ValueError, match="num_experts must be provided"):
        P.apply_ac(model)


def test_apply_ac_accepts_explicit_hidden_size_and_num_experts(monkeypatch):
    """Test that apply_ac accepts explicit hidden_size and num_experts parameters."""
    P = _import_parallelizer_with_stubs(monkeypatch)

    captured_hidden_size = None
    captured_num_experts = None

    def fake_create_selective_checkpoint_contexts(policy_cb):
        nonlocal captured_hidden_size, captured_num_experts
        torch_stub = sys.modules["torch"]
        # Test with the expected explicit values
        rhs_match = type("Mat", (), {"shape": (512, 32)})()
        result = policy_cb(None, torch_stub.ops.aten.mm.default, object(), rhs_match)
        if result == P.CheckpointPolicy.MUST_SAVE:
            captured_hidden_size = 512
            captured_num_experts = 32
        return "CTX"

    def fake_wrapper(block, preserve_rng_state, context_fn=None):
        if context_fn is not None:
            context_fn()
        return block

    monkeypatch.setattr(P, "create_selective_checkpoint_contexts", fake_create_selective_checkpoint_contexts)
    monkeypatch.setattr(P, "ptd_checkpoint_wrapper", MagicMock(side_effect=fake_wrapper))

    # Model without config - should work with explicit params
    class ModelWithoutConfig:
        def __init__(self):
            self.layers = LayerContainer([DummyBlock()])

    model = ModelWithoutConfig()

    P.apply_ac(model, ignore_router=True, hidden_size=512, num_experts=32)

    assert captured_hidden_size == 512
    assert captured_num_experts == 32


def test_apply_ac_explicit_params_override_config(monkeypatch):
    """Test that explicit hidden_size and num_experts override model.config values."""
    P = _import_parallelizer_with_stubs(monkeypatch)

    captured_hidden_size = None
    captured_num_experts = None

    def fake_create_selective_checkpoint_contexts(policy_cb):
        nonlocal captured_hidden_size, captured_num_experts
        torch_stub = sys.modules["torch"]
        # Test with explicit override values
        rhs_match = type("Mat", (), {"shape": (1024, 64)})()
        result = policy_cb(None, torch_stub.ops.aten.mm.default, object(), rhs_match)
        if result == P.CheckpointPolicy.MUST_SAVE:
            captured_hidden_size = 1024
            captured_num_experts = 64
        return "CTX"

    def fake_wrapper(block, preserve_rng_state, context_fn=None):
        if context_fn is not None:
            context_fn()
        return block

    monkeypatch.setattr(P, "create_selective_checkpoint_contexts", fake_create_selective_checkpoint_contexts)
    monkeypatch.setattr(P, "ptd_checkpoint_wrapper", MagicMock(side_effect=fake_wrapper))

    # Model with config
    class Config:
        hidden_size = 256
        num_experts = 16

    class ModelWithConfig:
        def __init__(self):
            self.config = Config()
            self.layers = LayerContainer([DummyBlock()])

    model = ModelWithConfig()

    # Explicit params should override config
    P.apply_ac(model, ignore_router=True, hidden_size=1024, num_experts=64)

    assert captured_hidden_size == 1024
    assert captured_num_experts == 64


def test_parallelize_model_passes_ignore_router_for_ac_to_apply_ac(monkeypatch):
    """Test that parallelize_model passes ignore_router_for_ac to apply_ac."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    apply_ac_mock = MagicMock()
    monkeypatch.setattr(P, "apply_ac", apply_ac_mock)
    monkeypatch.setattr(P, "apply_ep", MagicMock())
    monkeypatch.setattr(P, "apply_fsdp", MagicMock())

    world_mesh = FakeWorldMesh({("dp",): 2}, mesh_dim_names=["dp"])
    moe_mesh = None

    class Inner:
        def __init__(self):
            self.moe_config = type("MC", (), {"n_routed_experts": 4})()

    class Outer:
        def __init__(self):
            self.model = Inner()

    model = Outer()

    P.parallelize_model(
        model=model,
        world_mesh=world_mesh,
        moe_mesh=moe_mesh,
        dp_axis_names=("dp",),
        activation_checkpointing=True,
        ignore_router_for_ac=True,
    )

    # Verify apply_ac was called with ignore_router=True
    apply_ac_mock.assert_called_once()
    args, kwargs = apply_ac_mock.call_args
    assert args[0] is model
    assert kwargs.get("ignore_router") is True


def test_parallelize_model_ignore_router_for_ac_defaults_to_false(monkeypatch):
    """Test that parallelize_model defaults ignore_router_for_ac to False."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    apply_ac_mock = MagicMock()
    monkeypatch.setattr(P, "apply_ac", apply_ac_mock)
    monkeypatch.setattr(P, "apply_ep", MagicMock())
    monkeypatch.setattr(P, "apply_fsdp", MagicMock())

    world_mesh = FakeWorldMesh({("dp",): 2}, mesh_dim_names=["dp"])
    moe_mesh = None

    class Inner:
        def __init__(self):
            self.moe_config = type("MC", (), {"n_routed_experts": 4})()

    class Outer:
        def __init__(self):
            self.model = Inner()

    model = Outer()

    P.parallelize_model(
        model=model,
        world_mesh=world_mesh,
        moe_mesh=moe_mesh,
        dp_axis_names=("dp",),
        activation_checkpointing=True,
    )

    # Verify apply_ac was called with ignore_router=False (default)
    apply_ac_mock.assert_called_once()
    args, kwargs = apply_ac_mock.call_args
    assert kwargs.get("ignore_router") is False


# ============================================================================
# Tests for block.moe attribute handling (Step3p5 style models)
# ============================================================================


class DummyBlockWithMoeAttr:
    """Block with separate moe attribute (Step3p5 style)."""

    def __init__(self, moe=None, mlp=None):
        self.moe = moe
        self.mlp = mlp


def test_apply_ep_handles_block_with_moe_attribute(monkeypatch):
    """Test that apply_ep correctly handles blocks with 'moe' attribute (Step3p5 style)."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    # Patch MoE symbol for isinstance
    monkeypatch.setattr(P, "MoE", DummyMoE)
    parallelize_module_mock = MagicMock()
    monkeypatch.setattr(P, "parallelize_module", parallelize_module_mock)

    moe = DummyMoE()
    # Block has moe attribute instead of mlp
    block = DummyBlockWithMoeAttr(moe=moe, mlp=None)
    model = DummyModel([block])
    ep_mesh = type("Mesh", (), {"size": lambda self: 2})()

    P.apply_ep(model, ep_mesh)

    assert parallelize_module_mock.call_count == 1
    args, kwargs = parallelize_module_mock.call_args
    # Should use block.moe.experts, not block.mlp.experts
    assert kwargs["module"] is moe.experts
    assert kwargs["device_mesh"] is ep_mesh
    assert isinstance(kwargs["parallelize_plan"], P.ExpertParallel)


def test_apply_ep_prefers_moe_over_mlp(monkeypatch):
    """Test that apply_ep prefers block.moe over block.mlp when both exist."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    monkeypatch.setattr(P, "MoE", DummyMoE)
    parallelize_module_mock = MagicMock()
    monkeypatch.setattr(P, "parallelize_module", parallelize_module_mock)

    moe = DummyMoE()
    mlp = DummyMoE()  # A different MoE object
    block = DummyBlockWithMoeAttr(moe=moe, mlp=mlp)
    model = DummyModel([block])
    ep_mesh = type("Mesh", (), {"size": lambda self: 2})()

    P.apply_ep(model, ep_mesh)

    assert parallelize_module_mock.call_count == 1
    args, kwargs = parallelize_module_mock.call_args
    # Should use block.moe.experts (not block.mlp.experts)
    assert kwargs["module"] is moe.experts


def test_apply_ep_falls_back_to_mlp(monkeypatch):
    """Test that apply_ep falls back to block.mlp when block.moe doesn't exist."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    monkeypatch.setattr(P, "MoE", DummyMoE)
    parallelize_module_mock = MagicMock()
    monkeypatch.setattr(P, "parallelize_module", parallelize_module_mock)

    mlp = DummyMoE()
    # Block with mlp but no moe attribute
    block = DummyBlock(mlp=mlp)
    model = DummyModel([block])
    ep_mesh = type("Mesh", (), {"size": lambda self: 2})()

    P.apply_ep(model, ep_mesh)

    assert parallelize_module_mock.call_count == 1
    args, kwargs = parallelize_module_mock.call_args
    assert kwargs["module"] is mlp.experts


def test_apply_ac_derives_num_experts_from_moe_num_experts(monkeypatch):
    """Test that apply_ac derives num_experts from config.moe_num_experts when num_experts is absent."""
    P = _import_parallelizer_with_stubs(monkeypatch)

    captured_num_experts = None

    def fake_create_selective_checkpoint_contexts(policy_cb):
        nonlocal captured_num_experts
        torch_stub = sys.modules["torch"]
        # Test with various shapes to determine what was captured
        for ne in [8, 16, 32, 64]:
            rhs = type("Mat", (), {"shape": (256, ne)})()
            result = policy_cb(None, torch_stub.ops.aten.mm.default, object(), rhs)
            if result == P.CheckpointPolicy.MUST_SAVE:
                captured_num_experts = ne
                break
        return "CTX"

    def fake_wrapper(block, preserve_rng_state, context_fn=None):
        if context_fn is not None:
            context_fn()
        return block

    monkeypatch.setattr(P, "create_selective_checkpoint_contexts", fake_create_selective_checkpoint_contexts)
    monkeypatch.setattr(P, "ptd_checkpoint_wrapper", MagicMock(side_effect=fake_wrapper))

    # Create model with config containing only moe_num_experts (not num_experts)
    class Config:
        hidden_size = 256
        moe_num_experts = 32  # Only moe_num_experts, not num_experts

    class ModelWithMoeNumExperts:
        def __init__(self):
            self.config = Config()
            self.layers = LayerContainer([DummyBlock()])

    model = ModelWithMoeNumExperts()

    P.apply_ac(model, ignore_router=True)

    # Should find moe_num_experts
    assert captured_num_experts == 32


def test_apply_ac_prefers_num_experts_over_moe_num_experts(monkeypatch):
    """Test that apply_ac prefers config.num_experts over config.moe_num_experts."""
    P = _import_parallelizer_with_stubs(monkeypatch)

    captured_num_experts = None

    def fake_create_selective_checkpoint_contexts(policy_cb):
        nonlocal captured_num_experts
        torch_stub = sys.modules["torch"]
        for ne in [8, 16, 32, 64]:
            rhs = type("Mat", (), {"shape": (256, ne)})()
            result = policy_cb(None, torch_stub.ops.aten.mm.default, object(), rhs)
            if result == P.CheckpointPolicy.MUST_SAVE:
                captured_num_experts = ne
                break
        return "CTX"

    def fake_wrapper(block, preserve_rng_state, context_fn=None):
        if context_fn is not None:
            context_fn()
        return block

    monkeypatch.setattr(P, "create_selective_checkpoint_contexts", fake_create_selective_checkpoint_contexts)
    monkeypatch.setattr(P, "ptd_checkpoint_wrapper", MagicMock(side_effect=fake_wrapper))

    # Create model with both num_experts and moe_num_experts
    class Config:
        hidden_size = 256
        num_experts = 16  # Should be preferred
        moe_num_experts = 64  # Should be ignored

    class ModelWithBothExperts:
        def __init__(self):
            self.config = Config()
            self.layers = LayerContainer([DummyBlock()])

    model = ModelWithBothExperts()

    P.apply_ac(model, ignore_router=True)

    # Should find num_experts first
    assert captured_num_experts == 16


def test_apply_fsdp_handles_block_with_moe_attribute(monkeypatch):
    """Test that apply_fsdp correctly handles blocks with 'moe' attribute (Step3p5 style)."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    monkeypatch.setattr(P, "MoE", DummyMoE)

    fully_shard_mock = MagicMock()
    mp_policy_mock = MagicMock(return_value="MP_POLICY")
    shard_sentinel = object()

    def fake_shard(dim):
        assert dim == 1
        return shard_sentinel

    monkeypatch.setattr(P, "fully_shard", fully_shard_mock)
    monkeypatch.setattr(P, "MixedPrecisionPolicy", mp_policy_mock)
    monkeypatch.setattr(P, "Shard", fake_shard)

    moe = DummyMoE()
    block = DummyBlockWithMoeAttr(moe=moe, mlp=None)
    model = DummyModel([block])

    fsdp_mesh = type("Mesh", (), {"size": lambda self: 2})()
    ep_shard_mesh = type("Mesh", (), {"size": lambda self: 2})()

    P.apply_fsdp(
        model=model,
        fsdp_mesh=fsdp_mesh,
        ep_enabled=True,
        ep_shard_enabled=True,
        ep_shard_mesh=ep_shard_mesh,
    )

    # Experts should have a dedicated shard call using block.moe.experts
    experts = moe.experts
    experts_call = _find_call_by_first_arg(fully_shard_mock, experts)
    assert experts_call is not None
    _, experts_kwargs = experts_call
    assert experts_kwargs["mesh"] is ep_shard_mesh

    # Block should be sharded with ignored_params from moe.experts
    block_call = _find_call_by_first_arg(fully_shard_mock, block)
    assert block_call is not None
    _, block_kwargs = block_call
    ignored = block_kwargs.get("ignored_params")
    assert isinstance(ignored, set) and len(ignored) == len(list(experts.parameters()))


def test_apply_fsdp_uses_moe_for_ignored_params(monkeypatch):
    """Test that apply_fsdp uses block.moe.experts for ignored_params when ep_enabled."""
    P = _import_parallelizer_with_stubs(monkeypatch)
    monkeypatch.setattr(P, "MoE", DummyMoE)

    fully_shard_mock = MagicMock()
    monkeypatch.setattr(P, "fully_shard", fully_shard_mock)
    monkeypatch.setattr(P, "MixedPrecisionPolicy", MagicMock(return_value="MP_POLICY"))

    moe = DummyMoE()
    mlp = DummyMoE()  # Different object
    block = DummyBlockWithMoeAttr(moe=moe, mlp=mlp)
    model = DummyModel([block])
    fsdp_mesh = object()

    P.apply_fsdp(
        model=model,
        fsdp_mesh=fsdp_mesh,
        ep_enabled=True,
        ep_shard_enabled=False,
        ep_shard_mesh=None,
    )

    block_call = _find_call_by_first_arg(fully_shard_mock, block)
    assert block_call is not None
    _, block_kwargs = block_call
    ignored = block_kwargs.get("ignored_params")
    # Should use moe.experts, not mlp.experts
    assert isinstance(ignored, set)
    moe_params = set(moe.experts.parameters())
    assert ignored == moe_params
